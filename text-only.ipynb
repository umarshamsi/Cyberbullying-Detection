{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T13:47:15.725129Z","iopub.execute_input":"2021-05-29T13:47:15.725834Z","iopub.status.idle":"2021-05-29T13:47:17.705005Z","shell.execute_reply.started":"2021-05-29T13:47:15.725651Z","shell.execute_reply":"2021-05-29T13:47:17.704041Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Bidirectional, LSTM, dot, concatenate, Activation\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, LeakyReLU\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:17.707519Z","iopub.execute_input":"2021-05-29T13:47:17.708049Z","iopub.status.idle":"2021-05-29T13:47:17.716928Z","shell.execute_reply.started":"2021-05-29T13:47:17.708004Z","shell.execute_reply":"2021-05-29T13:47:17.715480Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# class TwoStreamBiLSTM:\n#     def __init__(self, hidden_size=1024, no_classes=1):\n#         self.hidden_size = hidden_size\n#         self.no_classes = no_classes\n\n#     def build(self, input_shape=((15, 512))):\n#         input_text =  Input(shape=input_shape)\n\n#         x = Bidirectional(LSTM(self.hidden_size, return_sequences=True, kernel_initializer='glorot_uniform'))(input_text)\n#         x = Bidirectional(LSTM(self.hidden_size, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n#         x = Bidirectional(LSTM(self.hidden_size*2, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n# #         x = Bidirectional(LSTM(self.hidden_size*2, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n# #         x = Bidirectional(LSTM(self.hidden_size*2, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n\n#         x1 = GlobalAveragePooling1D()(x)\n#         #x1 = GlobalMaxPooling1D()(x)\n\n#         x = Dense(self.hidden_size*2, kernel_initializer='glorot_uniform')(x1)\n#         x = LeakyReLU(0.2)(x)\n\n#         x = Dropout(rate=0.2)(x)\n\n#         output = Dense(self.no_classes, activation='sigmoid', kernel_initializer='glorot_uniform')(x)\n\n#         return Model(inputs=input_text, outputs=output)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:17.719260Z","iopub.execute_input":"2021-05-29T13:47:17.719876Z","iopub.status.idle":"2021-05-29T13:47:17.728519Z","shell.execute_reply.started":"2021-05-29T13:47:17.719818Z","shell.execute_reply":"2021-05-29T13:47:17.727283Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class TwoStreamBiLSTM:\n    #chng\n    def __init__(self, input_shape=(15, 768), hidden_states=256):\n        self.input_shape = input_shape\n        self.hidden_states = hidden_states\n    \n    def build(self):\n        inp = Input(shape=self.input_shape)\n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(inp)\n        x1 = LeakyReLU(0.2)(x)\n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x1)\n        x = LeakyReLU(0.2)(x)\n        \n        x = concatenate([x1, x])\n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n        x1 = LeakyReLU(0.2)(x)\n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x1)\n        x = LeakyReLU(0.2)(x)\n        \n        x = concatenate([x1, x])\n        \n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=False, kernel_initializer='glorot_uniform'))(x)\n        x = LeakyReLU(0.2)(x)\n        \n        x = Dense(512, kernel_initializer='glorot_uniform')(x)\n        x = LeakyReLU(0.2)(x)\n        x = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n        \n        model = Model(inputs=inp, outputs=x)\n        return model\n    def build_bilstm(self):\n        inp = Input(shape=self.input_shape)\n        \n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(inp)\n        x = Bidirectional(LSTM(self.hidden_states, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n        \n        x1 = GlobalMaxPooling1D(x)\n        x2 = GlobalAveragePooling1D(x)\n        \n        x = concatenate([x1, x2])\n        \n        x = Dense(512, kernel_initializer='glorot_uniform')(x)\n        x = LeakyReLU(0.2)(x)\n        x = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(x)\n        \n        model = Model(inputs=inp, outputs=x)\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:17.732229Z","iopub.execute_input":"2021-05-29T13:47:17.732550Z","iopub.status.idle":"2021-05-29T13:47:17.750044Z","shell.execute_reply.started":"2021-05-29T13:47:17.732504Z","shell.execute_reply":"2021-05-29T13:47:17.748416Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/bert-embeddings/postwise_comment_embedding_bert.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:17.754228Z","iopub.execute_input":"2021-05-29T13:47:17.754723Z","iopub.status.idle":"2021-05-29T13:47:19.455838Z","shell.execute_reply.started":"2021-05-29T13:47:17.754680Z","shell.execute_reply":"2021-05-29T13:47:19.454369Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      _unit_id  label                                          embedding\n0    698432761      0  [[-0.2728136479854584, -0.8870247602462769, -0...\n1    698432762      0  [[0.18904195725917816, -0.5301364660263062, 0....\n2    698432763      0  [[0.4969237148761749, -0.2632063627243042, 0.7...\n3    698432764      0  [[-0.13861827552318573, -0.779614269733429, 0....\n4    698432765      1  [[-0.3295409381389618, -0.29162853956222534, -...\n..         ...    ...                                                ...\n948  698447713      1  [[-0.4126046895980835, -0.2747979760169983, 0....\n949  698447782      1  [[0.38253065943717957, -0.13010767102241516, -...\n950  698447845      0  [[-0.15763697028160095, -0.7029473185539246, -...\n951  698447928      0  [[-0.40601351857185364, -0.4909651577472687, 0...\n952  698448129      0  [[0.30201297998428345, -0.7934238910675049, -0...\n\n[953 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_unit_id</th>\n      <th>label</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>698432761</td>\n      <td>0</td>\n      <td>[[-0.2728136479854584, -0.8870247602462769, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>698432762</td>\n      <td>0</td>\n      <td>[[0.18904195725917816, -0.5301364660263062, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>698432763</td>\n      <td>0</td>\n      <td>[[0.4969237148761749, -0.2632063627243042, 0.7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>698432764</td>\n      <td>0</td>\n      <td>[[-0.13861827552318573, -0.779614269733429, 0....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>698432765</td>\n      <td>1</td>\n      <td>[[-0.3295409381389618, -0.29162853956222534, -...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>948</th>\n      <td>698447713</td>\n      <td>1</td>\n      <td>[[-0.4126046895980835, -0.2747979760169983, 0....</td>\n    </tr>\n    <tr>\n      <th>949</th>\n      <td>698447782</td>\n      <td>1</td>\n      <td>[[0.38253065943717957, -0.13010767102241516, -...</td>\n    </tr>\n    <tr>\n      <th>950</th>\n      <td>698447845</td>\n      <td>0</td>\n      <td>[[-0.15763697028160095, -0.7029473185539246, -...</td>\n    </tr>\n    <tr>\n      <th>951</th>\n      <td>698447928</td>\n      <td>0</td>\n      <td>[[-0.40601351857185364, -0.4909651577472687, 0...</td>\n    </tr>\n    <tr>\n      <th>952</th>\n      <td>698448129</td>\n      <td>0</td>\n      <td>[[0.30201297998428345, -0.7934238910675049, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>953 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\ndef convert_str_to_array(array_str):\n    array_str = array_str.replace('[', '')\n    array_str = array_str.replace(']', '')\n    array_str = array_str.replace(' ', '')\n    return np.fromstring(array_str, sep=', ')\nfor i in range(len(df)):\n  arr = df['embedding'][i]\n  arr = convert_str_to_array(arr)\n  seq = []\n  for j in range(0,15):\n    #chng\n    seq.append(np.asarray(arr[j*768: j*768+768]))\n  df['embedding'][i]=np.asarray(seq)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:19.457952Z","iopub.execute_input":"2021-05-29T13:47:19.458848Z","iopub.status.idle":"2021-05-29T13:47:26.724356Z","shell.execute_reply.started":"2021-05-29T13:47:19.458802Z","shell.execute_reply":"2021-05-29T13:47:26.723342Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    _unit_id  label                                          embedding\n0  698432761      0  [[-0.2728136479854584, -0.8870247602462769, -0...\n1  698432762      0  [[0.18904195725917816, -0.5301364660263062, 0....\n2  698432763      0  [[0.4969237148761749, -0.2632063627243042, 0.7...\n3  698432764      0  [[-0.13861827552318573, -0.779614269733429, 0....\n4  698432765      1  [[-0.3295409381389618, -0.29162853956222534, -...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_unit_id</th>\n      <th>label</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>698432761</td>\n      <td>0</td>\n      <td>[[-0.2728136479854584, -0.8870247602462769, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>698432762</td>\n      <td>0</td>\n      <td>[[0.18904195725917816, -0.5301364660263062, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>698432763</td>\n      <td>0</td>\n      <td>[[0.4969237148761749, -0.2632063627243042, 0.7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>698432764</td>\n      <td>0</td>\n      <td>[[-0.13861827552318573, -0.779614269733429, 0....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>698432765</td>\n      <td>1</td>\n      <td>[[-0.3295409381389618, -0.29162853956222534, -...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n\nkf = KFold(n_splits=4,random_state = 43, shuffle=True)\n\nprecision_vals = []\nrecall_vals = []\nf1_vals = []\nacc_vals = []\nthresholds = []\n\ncount = 0\n\nfor i in range(1, 400):\n  thresholds.append(.002*i+.1)\n#print(thresholds)\n\nfor train_index, test_index in kf.split(df['embedding'].values):\n  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n  X_train, X_test = df['embedding'].values[train_index], df['embedding'].values[test_index]\n  y_train, y_test = df['label'].values[train_index], df['label'].values[test_index]\n  print(len(X_train))\n  print(len(X_test))\n\n  monitor = 'val_recall_at_precision'\n  if(count!=0):\n    monitor += ('_'+str(count))\n  #print(monitor)\n  count+=1\n\n  callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor= monitor, patience=10, min_delta=0, restore_best_weights=True, mode='max'),\n    #tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n    #tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, patience=2, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n]\n  model = TwoStreamBiLSTM()\n  model = model.build()\n  model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy', tf.keras.metrics.RecallAtPrecision(.70)])\n  # model.summary()\n\n  X = []\n  y = []\n  X_val = []\n  y_val = []\n\n  for i in range(len(X_train)):\n    try:\n      X.append(np.array(X_train[i],dtype=np.float32))\n      y.append(np.array([y_train[i]],dtype=np.float32))\n    except:\n      pass\n\n  for i in range(len(X_test)):\n    try:\n      X_val.append(np.array(X_test[i],dtype=np.float32))\n      y_val.append(np.array([y_test[i]],dtype=np.float32))\n    except:\n      pass\n\n#   print(len(X))\n#   print(len(y))\n  model.fit(np.array(X), np.array(y), epochs=50, validation_data = (np.array(X_val), np.array(y_val)), callbacks = callbacks)\n\n  X = []\n  y = []\n  for i in range(len(X_test)):\n    try:\n      X.append(np.array(X_test[i],dtype=np.float32))\n      y.append(np.array([y_test[i]],dtype=np.float32))\n    except:\n      pass\n  print(len(X))\n  print(len(y))\n  y_predicted = model.predict(np.array(X))\n  print(y_predicted[:10])\n\n\n  pvals = []\n  rvals = []\n  fvals = [] \n  avals = []\n\n  for threshold in thresholds:\n    y_pred = np.copy(y_predicted)\n    #print(y_pred[:10])\n    for i in range(len(y_pred)):\n      if y_pred[i][0]>=threshold:\n        y_pred[i][0]=1\n      else:\n        y_pred[i][0]=0\n    #print(y_pred[:10])\n\n    #matrix = confusion_matrix(y,y_pred)\n    #print(matrix)\n    #pre = precision_score(y,y_pred)\n    #print(\"Precision: \",pre)\n    #pvals.append(pre)\n    #rec = recall_score(y,y_pred)\n    #print(\"Recall: \",rec)\n    #rvals.append(rec)\n    f1 = f1_score(y, y_pred)\n    if(f1>.55):\n        print(round(threshold,3), \": \",f1, end=' ')\n    fvals.append(f1)\n    acc = accuracy_score(y, y_pred)\n    #print(\"Accuracy: \",acc)\n    avals.append(acc)\n  #precision_vals.append(pvals)\n  #recall_vals.append(rvals)\n  fvals.sort(reverse=True)\n  print('max_f1 = ', fvals[0])\n  f1_vals.append(fvals)\n  acc_vals.append(avals)\n\n#print(f1_vals)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:47:26.726177Z","iopub.execute_input":"2021-05-29T13:47:26.726621Z","iopub.status.idle":"2021-05-29T13:50:28.060544Z","shell.execute_reply.started":"2021-05-29T13:47:26.726579Z","shell.execute_reply":"2021-05-29T13:50:28.059505Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"714\n239\nEpoch 1/50\n23/23 [==============================] - 21s 225ms/step - loss: 0.6090 - accuracy: 0.6543 - recall_at_precision: 0.0196 - val_loss: 0.5134 - val_accuracy: 0.7615 - val_recall_at_precision: 0.4085\nEpoch 2/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.5308 - accuracy: 0.7698 - recall_at_precision: 0.3464 - val_loss: 0.5249 - val_accuracy: 0.7573 - val_recall_at_precision: 0.4507\nEpoch 3/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4915 - accuracy: 0.7553 - recall_at_precision: 0.4664 - val_loss: 0.5040 - val_accuracy: 0.7824 - val_recall_at_precision: 0.4507\nEpoch 4/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4569 - accuracy: 0.8232 - recall_at_precision: 0.7289 - val_loss: 0.5013 - val_accuracy: 0.7782 - val_recall_at_precision: 0.4930\nEpoch 5/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.4752 - accuracy: 0.7903 - recall_at_precision: 0.6637 - val_loss: 0.5316 - val_accuracy: 0.7782 - val_recall_at_precision: 0.5211\nEpoch 6/50\n23/23 [==============================] - 1s 43ms/step - loss: 0.4101 - accuracy: 0.8247 - recall_at_precision: 0.7799 - val_loss: 0.5214 - val_accuracy: 0.7615 - val_recall_at_precision: 0.4085\nEpoch 7/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.4031 - accuracy: 0.8385 - recall_at_precision: 0.8162 - val_loss: 0.5896 - val_accuracy: 0.6611 - val_recall_at_precision: 0.5634\nEpoch 8/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4318 - accuracy: 0.8094 - recall_at_precision: 0.7400 - val_loss: 0.5288 - val_accuracy: 0.7950 - val_recall_at_precision: 0.4648\nEpoch 9/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.3495 - accuracy: 0.8556 - recall_at_precision: 0.8863 - val_loss: 0.6056 - val_accuracy: 0.7406 - val_recall_at_precision: 0.2535\nEpoch 10/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.3802 - accuracy: 0.8456 - recall_at_precision: 0.8352 - val_loss: 0.6145 - val_accuracy: 0.7238 - val_recall_at_precision: 0.1972\nEpoch 11/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2713 - accuracy: 0.9117 - recall_at_precision: 0.9053 - val_loss: 0.6996 - val_accuracy: 0.7280 - val_recall_at_precision: 0.2958\nEpoch 12/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.2090 - accuracy: 0.9254 - recall_at_precision: 0.9765 - val_loss: 0.7677 - val_accuracy: 0.7782 - val_recall_at_precision: 0.3662\nEpoch 13/50\n23/23 [==============================] - 1s 44ms/step - loss: 0.1708 - accuracy: 0.9390 - recall_at_precision: 0.9797 - val_loss: 0.6717 - val_accuracy: 0.7322 - val_recall_at_precision: 0.2958\nEpoch 14/50\n23/23 [==============================] - 1s 38ms/step - loss: 0.1049 - accuracy: 0.9634 - recall_at_precision: 0.9931 - val_loss: 0.8393 - val_accuracy: 0.7448 - val_recall_at_precision: 0.1690\nEpoch 15/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1002 - accuracy: 0.9716 - recall_at_precision: 0.9952 - val_loss: 0.8533 - val_accuracy: 0.7322 - val_recall_at_precision: 0.3521\nEpoch 16/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0875 - accuracy: 0.9644 - recall_at_precision: 0.9882 - val_loss: 1.0519 - val_accuracy: 0.7615 - val_recall_at_precision: 0.3099\nEpoch 17/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1204 - accuracy: 0.9629 - recall_at_precision: 0.9844 - val_loss: 1.2042 - val_accuracy: 0.6820 - val_recall_at_precision: 0.2676\n239\n239\n[[0.07839049]\n [0.40580618]\n [0.7195044 ]\n [0.8295483 ]\n [0.77966374]\n [0.68919814]\n [0.36888844]\n [0.8364892 ]\n [0.64939654]\n [0.8132434 ]]\n0.31 :  0.5504587155963303 0.312 :  0.5504587155963303 0.314 :  0.5504587155963303 0.316 :  0.5504587155963303 0.318 :  0.5504587155963303 0.32 :  0.5504587155963303 0.322 :  0.5504587155963303 0.324 :  0.5504587155963303 0.326 :  0.5504587155963303 0.328 :  0.5504587155963303 0.33 :  0.5504587155963303 0.332 :  0.5504587155963303 0.334 :  0.5504587155963303 0.338 :  0.5514018691588785 0.34 :  0.5514018691588785 0.342 :  0.5514018691588785 0.344 :  0.5514018691588785 0.346 :  0.5539906103286386 0.348 :  0.5539906103286386 0.35 :  0.5539906103286386 0.352 :  0.5539906103286386 0.354 :  0.5566037735849055 0.356 :  0.5566037735849055 0.358 :  0.5566037735849055 0.36 :  0.5566037735849055 0.362 :  0.5566037735849055 0.364 :  0.5566037735849055 0.366 :  0.5566037735849055 0.368 :  0.5566037735849055 0.37 :  0.5619047619047619 0.372 :  0.5619047619047619 0.374 :  0.5619047619047619 0.376 :  0.5619047619047619 0.378 :  0.5619047619047619 0.38 :  0.5645933014354068 0.382 :  0.5576923076923077 0.384 :  0.5560975609756097 0.386 :  0.5560975609756097 0.388 :  0.5615763546798029 0.39 :  0.5643564356435644 0.392 :  0.5671641791044776 0.394 :  0.5671641791044776 0.396 :  0.5671641791044776 0.398 :  0.5671641791044776 0.4 :  0.5671641791044776 0.402 :  0.5671641791044776 0.404 :  0.5599999999999999 0.406 :  0.5656565656565656 0.408 :  0.5656565656565656 0.41 :  0.5583756345177664 0.412 :  0.5583756345177664 0.414 :  0.5583756345177664 0.416 :  0.5510204081632653 0.426 :  0.5549738219895287 0.428 :  0.5549738219895287 0.43 :  0.5549738219895287 0.432 :  0.5549738219895287 0.434 :  0.5549738219895287 0.436 :  0.5578947368421052 0.438 :  0.5578947368421052 0.44 :  0.5578947368421052 0.442 :  0.5578947368421052 0.444 :  0.5608465608465608 0.446 :  0.5608465608465608 0.448 :  0.5638297872340425 0.45 :  0.5668449197860962 0.452 :  0.5668449197860962 0.454 :  0.5668449197860962 0.456 :  0.5668449197860962 0.458 :  0.5591397849462366 0.46 :  0.5591397849462366 0.462 :  0.5591397849462366 0.464 :  0.5591397849462366 0.466 :  0.5591397849462366 0.468 :  0.5591397849462366 0.47 :  0.5591397849462366 0.472 :  0.5591397849462366 0.474 :  0.5591397849462366 0.476 :  0.5591397849462366 0.478 :  0.5591397849462366 0.48 :  0.5591397849462366 0.482 :  0.5621621621621622 0.484 :  0.5621621621621622 0.486 :  0.5621621621621622 0.488 :  0.5621621621621622 0.49 :  0.5621621621621622 0.492 :  0.5621621621621622 0.494 :  0.5621621621621622 0.496 :  0.5543478260869565 0.498 :  0.5543478260869565 0.5 :  0.5573770491803278 0.502 :  0.5573770491803278 0.504 :  0.5604395604395604 0.506 :  0.5604395604395604 0.508 :  0.5524861878453038 0.618 :  0.5512820512820512 0.62 :  0.5512820512820512 0.622 :  0.5548387096774193 0.624 :  0.5584415584415584 0.626 :  0.5620915032679739 0.628 :  0.5620915032679739 0.63 :  0.5620915032679739 0.632 :  0.5695364238410596 0.634 :  0.5733333333333334 0.636 :  0.5733333333333334 0.638 :  0.581081081081081 0.64 :  0.581081081081081 0.642 :  0.581081081081081 0.644 :  0.581081081081081 0.646 :  0.581081081081081 0.648 :  0.5850340136054422 0.65 :  0.589041095890411 0.652 :  0.589041095890411 0.654 :  0.589041095890411 0.656 :  0.589041095890411 0.658 :  0.589041095890411 0.66 :  0.589041095890411 0.662 :  0.589041095890411 0.664 :  0.5793103448275861 0.666 :  0.5793103448275861 0.668 :  0.5793103448275861 0.67 :  0.5833333333333334 0.672 :  0.5874125874125875 0.674 :  0.5915492957746479 0.676 :  0.5957446808510638 0.678 :  0.6 0.68 :  0.6 0.682 :  0.6 0.684 :  0.6 0.686 :  0.6 0.688 :  0.60431654676259 0.69 :  0.608695652173913 0.692 :  0.608695652173913 0.694 :  0.608695652173913 0.696 :  0.5985401459854015 0.698 :  0.6029411764705882 0.7 :  0.6029411764705882 0.702 :  0.6029411764705882 0.704 :  0.6029411764705882 0.706 :  0.6029411764705882 0.708 :  0.6074074074074073 0.71 :  0.6074074074074073 0.712 :  0.6212121212121212 0.714 :  0.6153846153846153 0.716 :  0.6201550387596899 0.718 :  0.625 0.72 :  0.6299212598425197 0.722 :  0.6299212598425197 0.724 :  0.6299212598425197 0.726 :  0.6190476190476191 0.728 :  0.6190476190476191 0.73 :  0.6129032258064515 0.732 :  0.6129032258064515 0.734 :  0.6016260162601627 0.736 :  0.6016260162601627 0.738 :  0.5901639344262296 0.74 :  0.5785123966942148 0.742 :  0.576271186440678 0.744 :  0.576271186440678 0.746 :  0.576271186440678 0.748 :  0.576271186440678 0.75 :  0.5862068965517241 0.752 :  0.5535714285714285 max_f1 =  0.6299212598425197\n715\n238\nEpoch 1/50\n23/23 [==============================] - 19s 211ms/step - loss: 0.6218 - accuracy: 0.6882 - recall_at_precision_1: 0.0417 - val_loss: 0.5519 - val_accuracy: 0.7395 - val_recall_at_precision_1: 0.1867\nEpoch 2/50\n23/23 [==============================] - 1s 45ms/step - loss: 0.5100 - accuracy: 0.7639 - recall_at_precision_1: 0.5503 - val_loss: 0.5444 - val_accuracy: 0.7101 - val_recall_at_precision_1: 0.1733\nEpoch 3/50\n23/23 [==============================] - 1s 38ms/step - loss: 0.5080 - accuracy: 0.7486 - recall_at_precision_1: 0.4066 - val_loss: 0.5472 - val_accuracy: 0.7185 - val_recall_at_precision_1: 0.1600\nEpoch 4/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4737 - accuracy: 0.7827 - recall_at_precision_1: 0.5606 - val_loss: 0.5211 - val_accuracy: 0.7437 - val_recall_at_precision_1: 0.2267\nEpoch 5/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4737 - accuracy: 0.7992 - recall_at_precision_1: 0.7365 - val_loss: 0.5735 - val_accuracy: 0.7311 - val_recall_at_precision_1: 0.2933\nEpoch 6/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.4310 - accuracy: 0.8434 - recall_at_precision_1: 0.7209 - val_loss: 0.5243 - val_accuracy: 0.7689 - val_recall_at_precision_1: 0.3733\nEpoch 7/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.4552 - accuracy: 0.8223 - recall_at_precision_1: 0.7279 - val_loss: 0.5522 - val_accuracy: 0.7689 - val_recall_at_precision_1: 0.5733\nEpoch 8/50\n23/23 [==============================] - 1s 46ms/step - loss: 0.3793 - accuracy: 0.8440 - recall_at_precision_1: 0.7970 - val_loss: 0.5338 - val_accuracy: 0.7563 - val_recall_at_precision_1: 0.4933\nEpoch 9/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.3188 - accuracy: 0.8748 - recall_at_precision_1: 0.8649 - val_loss: 0.7165 - val_accuracy: 0.6303 - val_recall_at_precision_1: 0.4667\nEpoch 10/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.3584 - accuracy: 0.8360 - recall_at_precision_1: 0.8109 - val_loss: 0.9181 - val_accuracy: 0.7479 - val_recall_at_precision_1: 0.5333\nEpoch 11/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2573 - accuracy: 0.9165 - recall_at_precision_1: 0.9367 - val_loss: 0.6195 - val_accuracy: 0.7605 - val_recall_at_precision_1: 0.4533\nEpoch 12/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1866 - accuracy: 0.9419 - recall_at_precision_1: 0.9727 - val_loss: 0.6791 - val_accuracy: 0.7521 - val_recall_at_precision_1: 0.4133\nEpoch 13/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1751 - accuracy: 0.9465 - recall_at_precision_1: 0.9741 - val_loss: 0.6402 - val_accuracy: 0.7101 - val_recall_at_precision_1: 0.2667\nEpoch 14/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1552 - accuracy: 0.9543 - recall_at_precision_1: 0.9891 - val_loss: 0.7095 - val_accuracy: 0.7521 - val_recall_at_precision_1: 0.3867\nEpoch 15/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1330 - accuracy: 0.9474 - recall_at_precision_1: 0.9896 - val_loss: 0.8699 - val_accuracy: 0.7479 - val_recall_at_precision_1: 0.2667\nEpoch 16/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0482 - accuracy: 0.9864 - recall_at_precision_1: 1.0000 - val_loss: 1.0686 - val_accuracy: 0.6765 - val_recall_at_precision_1: 0.0933\nEpoch 17/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.0545 - accuracy: 0.9835 - recall_at_precision_1: 1.0000 - val_loss: 1.0027 - val_accuracy: 0.6975 - val_recall_at_precision_1: 0.3200\n238\n238\n[[0.03786728]\n [0.35320014]\n [0.09207468]\n [0.12723155]\n [0.78773904]\n [0.89906305]\n [0.8789265 ]\n [0.8099975 ]\n [0.89097744]\n [0.8918129 ]]\n0.128 :  0.5531914893617021 0.13 :  0.5555555555555555 0.132 :  0.5555555555555555 0.134 :  0.5555555555555555 0.136 :  0.5579399141630902 0.138 :  0.5579399141630902 0.14 :  0.5627705627705628 0.142 :  0.5652173913043478 0.144 :  0.5652173913043478 0.146 :  0.5701754385964912 0.148 :  0.5726872246696035 0.15 :  0.5726872246696035 0.152 :  0.5726872246696035 0.154 :  0.5752212389380531 0.156 :  0.5650224215246635 0.158 :  0.5701357466063348 0.16 :  0.5727272727272726 0.162 :  0.5662100456621004 0.164 :  0.5662100456621004 0.166 :  0.5688073394495413 0.168 :  0.5688073394495413 0.17 :  0.5688073394495413 0.172 :  0.5688073394495413 0.174 :  0.5688073394495413 0.176 :  0.5688073394495413 0.178 :  0.5688073394495413 0.18 :  0.5714285714285714 0.182 :  0.5714285714285714 0.184 :  0.5740740740740741 0.186 :  0.5740740740740741 0.188 :  0.5767441860465117 0.19 :  0.5794392523364487 0.192 :  0.5821596244131455 0.194 :  0.5821596244131455 0.196 :  0.5821596244131455 0.198 :  0.5849056603773585 0.2 :  0.5876777251184834 0.202 :  0.5876777251184834 0.204 :  0.5876777251184834 0.206 :  0.5876777251184834 0.208 :  0.5904761904761905 0.21 :  0.5904761904761905 0.212 :  0.5904761904761905 0.214 :  0.5933014354066986 0.216 :  0.5933014354066986 0.218 :  0.5990338164251208 0.22 :  0.6019417475728155 0.222 :  0.6019417475728155 0.224 :  0.5951219512195122 0.226 :  0.5951219512195122 0.228 :  0.6009852216748768 0.23 :  0.6009852216748768 0.232 :  0.6009852216748768 0.234 :  0.6009852216748768 0.236 :  0.6069651741293532 0.238 :  0.6069651741293532 0.24 :  0.6069651741293532 0.242 :  0.6069651741293532 0.244 :  0.6030150753768845 0.246 :  0.6030150753768845 0.248 :  0.6030150753768845 0.25 :  0.6030150753768845 0.252 :  0.6030150753768845 0.254 :  0.6030150753768845 0.256 :  0.6030150753768845 0.258 :  0.6060606060606061 0.26 :  0.598984771573604 0.262 :  0.598984771573604 0.264 :  0.5948717948717949 0.266 :  0.5948717948717949 0.268 :  0.5948717948717949 0.27 :  0.5948717948717949 0.272 :  0.5948717948717949 0.274 :  0.5948717948717949 0.276 :  0.5948717948717949 0.278 :  0.5948717948717949 0.28 :  0.5948717948717949 0.282 :  0.5979381443298969 0.284 :  0.5906735751295337 0.286 :  0.5906735751295337 0.288 :  0.5906735751295337 0.29 :  0.5906735751295337 0.292 :  0.5863874345549739 0.294 :  0.5894736842105264 0.296 :  0.5894736842105264 0.298 :  0.5894736842105264 0.3 :  0.5894736842105264 0.302 :  0.582010582010582 0.304 :  0.582010582010582 0.306 :  0.582010582010582 0.308 :  0.582010582010582 0.31 :  0.5851063829787234 0.312 :  0.5851063829787234 0.314 :  0.5851063829787234 0.316 :  0.5882352941176471 0.318 :  0.5882352941176471 0.32 :  0.5882352941176471 0.322 :  0.5882352941176471 0.324 :  0.5882352941176471 0.326 :  0.5882352941176471 0.328 :  0.5882352941176471 0.33 :  0.5882352941176471 0.332 :  0.5882352941176471 0.334 :  0.5913978494623656 0.336 :  0.5913978494623656 0.338 :  0.5913978494623656 0.34 :  0.5945945945945945 0.342 :  0.5945945945945945 0.344 :  0.5945945945945945 0.346 :  0.5945945945945945 0.348 :  0.5978260869565216 0.35 :  0.6010928961748634 0.352 :  0.6043956043956042 0.354 :  0.6077348066298343 0.356 :  0.6077348066298343 0.358 :  0.611111111111111 0.36 :  0.611111111111111 0.362 :  0.611111111111111 0.364 :  0.611111111111111 0.366 :  0.611111111111111 0.368 :  0.611111111111111 0.37 :  0.611111111111111 0.372 :  0.6067415730337079 0.374 :  0.6067415730337079 0.376 :  0.6067415730337079 0.378 :  0.6067415730337079 0.38 :  0.6067415730337079 0.382 :  0.6136363636363636 0.384 :  0.6136363636363636 0.386 :  0.6171428571428572 0.388 :  0.6171428571428572 0.39 :  0.6171428571428572 0.392 :  0.6171428571428572 0.394 :  0.6171428571428572 0.396 :  0.6127167630057805 0.398 :  0.6127167630057805 0.4 :  0.6127167630057805 0.402 :  0.6127167630057805 0.404 :  0.6127167630057805 0.406 :  0.6162790697674418 0.408 :  0.6162790697674418 0.41 :  0.6162790697674418 0.412 :  0.6162790697674418 0.414 :  0.6162790697674418 0.416 :  0.6162790697674418 0.418 :  0.6162790697674418 0.42 :  0.6198830409356726 0.422 :  0.6235294117647059 0.424 :  0.6235294117647059 0.426 :  0.6272189349112426 0.428 :  0.6265060240963856 0.43 :  0.6265060240963856 0.432 :  0.6341463414634146 0.434 :  0.638036809815951 0.436 :  0.638036809815951 0.438 :  0.638036809815951 0.44 :  0.638036809815951 0.442 :  0.6419753086419753 0.444 :  0.6419753086419753 0.446 :  0.6419753086419753 0.448 :  0.6419753086419753 0.45 :  0.6419753086419753 0.452 :  0.6419753086419753 0.454 :  0.6419753086419753 0.456 :  0.6419753086419753 0.458 :  0.6419753086419753 0.46 :  0.6459627329192547 0.462 :  0.6459627329192547 0.464 :  0.6459627329192547 0.466 :  0.6459627329192547 0.468 :  0.65 0.47 :  0.6415094339622641 0.472 :  0.6415094339622641 0.474 :  0.6415094339622641 0.476 :  0.6415094339622641 0.478 :  0.6415094339622641 0.48 :  0.6415094339622641 0.482 :  0.6415094339622641 0.484 :  0.6415094339622641 0.486 :  0.6455696202531646 0.488 :  0.6455696202531646 0.49 :  0.6496815286624205 0.492 :  0.6496815286624205 0.494 :  0.6496815286624205 0.496 :  0.6496815286624205 0.498 :  0.6496815286624205 0.5 :  0.6496815286624205 0.502 :  0.6496815286624205 0.504 :  0.6496815286624205 0.506 :  0.6496815286624205 0.508 :  0.6496815286624205 0.51 :  0.6496815286624205 0.512 :  0.6496815286624205 0.514 :  0.6496815286624205 0.516 :  0.6538461538461539 0.518 :  0.6538461538461539 0.52 :  0.6538461538461539 0.522 :  0.6538461538461539 0.524 :  0.6538461538461539 0.526 :  0.6538461538461539 0.528 :  0.6538461538461539 0.53 :  0.6538461538461539 0.532 :  0.6538461538461539 0.534 :  0.6538461538461539 0.536 :  0.6538461538461539 0.538 :  0.6538461538461539 0.54 :  0.6538461538461539 0.542 :  0.6451612903225806 0.544 :  0.6451612903225806 0.546 :  0.6451612903225806 0.548 :  0.6363636363636364 0.55 :  0.6363636363636364 0.552 :  0.6363636363636364 0.554 :  0.6363636363636364 0.556 :  0.6363636363636364 0.558 :  0.6363636363636364 0.56 :  0.6363636363636364 0.562 :  0.6274509803921569 0.564 :  0.6274509803921569 0.566 :  0.6274509803921569 0.568 :  0.618421052631579 0.57 :  0.618421052631579 0.572 :  0.6225165562913908 0.574 :  0.6225165562913908 0.576 :  0.6225165562913908 0.578 :  0.6225165562913908 0.58 :  0.6225165562913908 0.582 :  0.6225165562913908 0.584 :  0.6225165562913908 0.586 :  0.6225165562913908 0.588 :  0.6225165562913908 0.59 :  0.6225165562913908 0.592 :  0.6225165562913908 0.594 :  0.6225165562913908 0.596 :  0.6225165562913908 0.598 :  0.6225165562913908 0.6 :  0.6225165562913908 0.602 :  0.6225165562913908 0.604 :  0.6225165562913908 0.606 :  0.6225165562913908 0.608 :  0.6225165562913908 0.61 :  0.6225165562913908 0.612 :  0.6225165562913908 0.614 :  0.6266666666666667 0.616 :  0.6266666666666667 0.618 :  0.6266666666666667 0.62 :  0.6308724832214765 0.622 :  0.6308724832214765 0.624 :  0.6308724832214765 0.626 :  0.6308724832214765 0.628 :  0.6308724832214765 0.63 :  0.6216216216216216 0.632 :  0.6216216216216216 0.634 :  0.6216216216216216 0.636 :  0.6216216216216216 0.638 :  0.6216216216216216 0.64 :  0.6216216216216216 0.642 :  0.6216216216216216 0.644 :  0.6216216216216216 0.646 :  0.6216216216216216 0.648 :  0.6216216216216216 0.65 :  0.6216216216216216 0.652 :  0.6216216216216216 0.654 :  0.6216216216216216 0.656 :  0.6164383561643835 0.658 :  0.6164383561643835 0.66 :  0.6164383561643835 0.662 :  0.6164383561643835 0.664 :  0.6206896551724138 0.666 :  0.6293706293706294 0.668 :  0.6293706293706294 0.67 :  0.6293706293706294 0.672 :  0.6293706293706294 0.674 :  0.6293706293706294 0.676 :  0.6293706293706294 0.678 :  0.6293706293706294 0.68 :  0.6338028169014084 0.682 :  0.6338028169014084 0.684 :  0.6338028169014084 0.686 :  0.6338028169014084 0.688 :  0.6338028169014084 0.69 :  0.6382978723404256 0.692 :  0.6382978723404256 0.694 :  0.6285714285714286 0.696 :  0.6285714285714286 0.698 :  0.6285714285714286 0.7 :  0.6285714285714286 0.702 :  0.6285714285714286 0.704 :  0.6285714285714286 0.706 :  0.6285714285714286 0.708 :  0.6285714285714286 0.71 :  0.6231884057971014 0.712 :  0.6323529411764707 0.714 :  0.6323529411764707 0.716 :  0.6222222222222222 0.718 :  0.6222222222222222 0.72 :  0.6222222222222222 0.722 :  0.6222222222222222 0.724 :  0.6222222222222222 0.726 :  0.6222222222222222 0.728 :  0.6222222222222222 0.73 :  0.6222222222222222 0.732 :  0.6119402985074626 0.734 :  0.6119402985074626 0.736 :  0.6119402985074626 0.738 :  0.6119402985074626 0.74 :  0.6119402985074626 0.742 :  0.5909090909090909 0.744 :  0.5909090909090909 0.746 :  0.5909090909090909 0.748 :  0.5909090909090909 0.75 :  0.5801526717557253 0.752 :  0.5846153846153845 0.754 :  0.5846153846153845 0.756 :  0.5846153846153845 0.758 :  0.5846153846153845 0.76 :  0.5736434108527132 0.762 :  0.5625000000000001 0.764 :  0.5625000000000001 0.766 :  0.5625000000000001 0.768 :  0.5625000000000001 0.77 :  0.5625000000000001 0.772 :  0.56 0.774 :  0.56 0.776 :  0.56 0.778 :  0.5691056910569106 0.78 :  0.5691056910569106 0.782 :  0.5573770491803279 0.784 :  0.5573770491803279 0.786 :  0.5573770491803279 max_f1 =  0.6538461538461539\n715\n238\nEpoch 1/50\n23/23 [==============================] - 21s 233ms/step - loss: 0.6229 - accuracy: 0.6679 - recall_at_precision_2: 0.0402 - val_loss: 0.5804 - val_accuracy: 0.6639 - val_recall_at_precision_2: 0.4125\nEpoch 2/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.5624 - accuracy: 0.7137 - recall_at_precision_2: 0.2028 - val_loss: 0.5428 - val_accuracy: 0.7395 - val_recall_at_precision_2: 0.4875\nEpoch 3/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.5735 - accuracy: 0.7411 - recall_at_precision_2: 0.1689 - val_loss: 0.5291 - val_accuracy: 0.7437 - val_recall_at_precision_2: 0.5125\nEpoch 4/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.5177 - accuracy: 0.7701 - recall_at_precision_2: 0.3105 - val_loss: 0.5018 - val_accuracy: 0.7815 - val_recall_at_precision_2: 0.5250\nEpoch 5/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.5196 - accuracy: 0.7688 - recall_at_precision_2: 0.3826 - val_loss: 0.5877 - val_accuracy: 0.7437 - val_recall_at_precision_2: 0.6125\nEpoch 6/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.5032 - accuracy: 0.7889 - recall_at_precision_2: 0.5066 - val_loss: 0.5612 - val_accuracy: 0.7437 - val_recall_at_precision_2: 0.6250\nEpoch 7/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.4906 - accuracy: 0.7926 - recall_at_precision_2: 0.6549 - val_loss: 0.5301 - val_accuracy: 0.7269 - val_recall_at_precision_2: 0.6750\nEpoch 8/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4774 - accuracy: 0.7927 - recall_at_precision_2: 0.7271 - val_loss: 0.5438 - val_accuracy: 0.7563 - val_recall_at_precision_2: 0.4625\nEpoch 9/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.4482 - accuracy: 0.8250 - recall_at_precision_2: 0.7328 - val_loss: 0.4724 - val_accuracy: 0.7983 - val_recall_at_precision_2: 0.7625\nEpoch 10/50\n23/23 [==============================] - 1s 43ms/step - loss: 0.4560 - accuracy: 0.7934 - recall_at_precision_2: 0.5609 - val_loss: 0.4925 - val_accuracy: 0.7563 - val_recall_at_precision_2: 0.7125\nEpoch 11/50\n23/23 [==============================] - 1s 37ms/step - loss: 0.4053 - accuracy: 0.8450 - recall_at_precision_2: 0.6806 - val_loss: 0.5096 - val_accuracy: 0.7773 - val_recall_at_precision_2: 0.6125\nEpoch 12/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.3405 - accuracy: 0.8756 - recall_at_precision_2: 0.8162 - val_loss: 0.5030 - val_accuracy: 0.7521 - val_recall_at_precision_2: 0.5125\nEpoch 13/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.3441 - accuracy: 0.8786 - recall_at_precision_2: 0.8227 - val_loss: 0.5790 - val_accuracy: 0.7731 - val_recall_at_precision_2: 0.5875\nEpoch 14/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.2588 - accuracy: 0.9145 - recall_at_precision_2: 0.8280 - val_loss: 0.8501 - val_accuracy: 0.7563 - val_recall_at_precision_2: 0.4375\nEpoch 15/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.3243 - accuracy: 0.8907 - recall_at_precision_2: 0.8092 - val_loss: 0.5992 - val_accuracy: 0.7101 - val_recall_at_precision_2: 0.5250\nEpoch 16/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2887 - accuracy: 0.9047 - recall_at_precision_2: 0.8481 - val_loss: 0.6721 - val_accuracy: 0.7605 - val_recall_at_precision_2: 0.4625\nEpoch 17/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2318 - accuracy: 0.9285 - recall_at_precision_2: 0.8807 - val_loss: 0.7043 - val_accuracy: 0.7269 - val_recall_at_precision_2: 0.5625\nEpoch 18/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2384 - accuracy: 0.9162 - recall_at_precision_2: 0.9092 - val_loss: 0.9155 - val_accuracy: 0.7689 - val_recall_at_precision_2: 0.5375\nEpoch 19/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2106 - accuracy: 0.9330 - recall_at_precision_2: 0.8905 - val_loss: 0.7025 - val_accuracy: 0.7563 - val_recall_at_precision_2: 0.6625\n238\n238\n[[0.10874572]\n [0.8764033 ]\n [0.890634  ]\n [0.10560437]\n [0.10969551]\n [0.16745365]\n [0.9225133 ]\n [0.8993178 ]\n [0.91089004]\n [0.87461287]]\n0.106 :  0.5683453237410072 0.108 :  0.5808823529411765 0.11 :  0.5811320754716981 0.112 :  0.5855513307984791 0.114 :  0.5984251968503937 0.116 :  0.6129032258064516 0.118 :  0.6122448979591837 0.12 :  0.6115702479338843 0.122 :  0.6166666666666667 0.124 :  0.6186440677966102 0.126 :  0.6239316239316239 0.128 :  0.6266094420600858 0.13 :  0.6320346320346321 0.132 :  0.6347826086956522 0.134 :  0.6375545851528385 0.136 :  0.6403508771929824 0.138 :  0.6431718061674009 0.14 :  0.64 0.142 :  0.64 0.144 :  0.6486486486486486 0.146 :  0.6486486486486486 0.148 :  0.6486486486486486 0.15 :  0.6575342465753425 0.152 :  0.6575342465753425 0.154 :  0.6575342465753425 0.156 :  0.6575342465753425 0.158 :  0.6575342465753425 0.16 :  0.6605504587155964 0.162 :  0.6605504587155964 0.164 :  0.6666666666666667 0.166 :  0.6666666666666667 0.168 :  0.6697674418604651 0.17 :  0.6728971962616822 0.172 :  0.6728971962616822 0.174 :  0.6666666666666667 0.176 :  0.6698113207547169 0.178 :  0.6698113207547169 0.18 :  0.6698113207547169 0.182 :  0.6698113207547169 0.184 :  0.6698113207547169 0.186 :  0.6698113207547169 0.188 :  0.6698113207547169 0.19 :  0.6698113207547169 0.192 :  0.6698113207547169 0.194 :  0.6698113207547169 0.196 :  0.6698113207547169 0.198 :  0.6666666666666667 0.2 :  0.6666666666666667 0.202 :  0.6666666666666667 0.204 :  0.6666666666666667 0.206 :  0.6698564593301435 0.208 :  0.6730769230769231 0.21 :  0.6730769230769231 0.212 :  0.6730769230769231 0.214 :  0.6730769230769231 0.216 :  0.6730769230769231 0.218 :  0.6730769230769231 0.22 :  0.6730769230769231 0.222 :  0.6730769230769231 0.224 :  0.6730769230769231 0.226 :  0.6730769230769231 0.228 :  0.6730769230769231 0.23 :  0.6829268292682927 0.232 :  0.6829268292682927 0.234 :  0.6829268292682927 0.236 :  0.6829268292682927 0.238 :  0.6798029556650247 0.24 :  0.6798029556650247 0.242 :  0.6831683168316832 0.244 :  0.6831683168316832 0.246 :  0.6831683168316832 0.248 :  0.6831683168316832 0.25 :  0.6865671641791046 0.252 :  0.6865671641791046 0.254 :  0.6865671641791046 0.256 :  0.69 0.258 :  0.6934673366834172 0.26 :  0.6969696969696969 0.262 :  0.6969696969696969 0.264 :  0.6903553299492386 0.266 :  0.6903553299492386 0.268 :  0.6903553299492386 0.27 :  0.6903553299492386 0.272 :  0.6938775510204082 0.274 :  0.6938775510204082 0.276 :  0.6938775510204082 0.278 :  0.6938775510204082 0.28 :  0.6871794871794872 0.282 :  0.6871794871794872 0.284 :  0.6907216494845361 0.286 :  0.6907216494845361 0.288 :  0.6839378238341969 0.29 :  0.6839378238341969 0.292 :  0.6839378238341969 0.294 :  0.6875000000000001 0.296 :  0.6875000000000001 0.298 :  0.6875000000000001 0.3 :  0.6875000000000001 0.302 :  0.6875000000000001 0.304 :  0.6875000000000001 0.306 :  0.6875000000000001 0.308 :  0.6875000000000001 0.31 :  0.6947368421052632 0.312 :  0.6984126984126984 0.314 :  0.6984126984126984 0.316 :  0.6984126984126984 0.318 :  0.6984126984126984 0.32 :  0.6984126984126984 0.322 :  0.6984126984126984 0.324 :  0.6984126984126984 0.326 :  0.6984126984126984 0.328 :  0.6984126984126984 0.33 :  0.6984126984126984 0.332 :  0.7021276595744681 0.334 :  0.7021276595744681 0.336 :  0.7021276595744681 0.338 :  0.7021276595744681 0.34 :  0.7021276595744681 0.342 :  0.7021276595744681 0.344 :  0.7021276595744681 0.346 :  0.7021276595744681 0.348 :  0.7021276595744681 0.35 :  0.7021276595744681 0.352 :  0.7021276595744681 0.354 :  0.7021276595744681 0.356 :  0.7021276595744681 0.358 :  0.7021276595744681 0.36 :  0.7021276595744681 0.362 :  0.7021276595744681 0.364 :  0.7021276595744681 0.366 :  0.7021276595744681 0.368 :  0.7058823529411763 0.37 :  0.7058823529411763 0.372 :  0.7058823529411763 0.374 :  0.7058823529411763 0.376 :  0.7096774193548386 0.378 :  0.7135135135135136 0.38 :  0.7135135135135136 0.382 :  0.7135135135135136 0.384 :  0.7135135135135136 0.386 :  0.7135135135135136 0.388 :  0.7135135135135136 0.39 :  0.7135135135135136 0.392 :  0.7135135135135136 0.394 :  0.7135135135135136 0.396 :  0.7065217391304348 0.398 :  0.7065217391304348 0.4 :  0.7065217391304348 0.402 :  0.7065217391304348 0.404 :  0.7065217391304348 0.406 :  0.7065217391304348 0.408 :  0.7065217391304348 0.41 :  0.7065217391304348 0.412 :  0.7065217391304348 0.414 :  0.7065217391304348 0.416 :  0.7065217391304348 0.418 :  0.7065217391304348 0.42 :  0.7065217391304348 0.422 :  0.7065217391304348 0.424 :  0.7065217391304348 0.426 :  0.7065217391304348 0.428 :  0.7065217391304348 0.43 :  0.7065217391304348 0.432 :  0.7065217391304348 0.434 :  0.7142857142857142 0.436 :  0.7142857142857142 0.438 :  0.7142857142857142 0.44 :  0.7142857142857142 0.442 :  0.7142857142857142 0.444 :  0.7142857142857142 0.446 :  0.7142857142857142 0.448 :  0.7142857142857142 0.45 :  0.7142857142857142 0.452 :  0.7142857142857142 0.454 :  0.718232044198895 0.456 :  0.718232044198895 0.458 :  0.718232044198895 0.46 :  0.718232044198895 0.462 :  0.718232044198895 0.464 :  0.718232044198895 0.466 :  0.718232044198895 0.468 :  0.718232044198895 0.47 :  0.718232044198895 0.472 :  0.718232044198895 0.474 :  0.7222222222222223 0.476 :  0.7222222222222223 0.478 :  0.7222222222222223 0.48 :  0.7222222222222223 0.482 :  0.7222222222222223 0.484 :  0.7222222222222223 0.486 :  0.7222222222222223 0.488 :  0.7262569832402236 0.49 :  0.7303370786516853 0.492 :  0.7344632768361582 0.494 :  0.7344632768361582 0.496 :  0.7344632768361582 0.498 :  0.7272727272727272 0.5 :  0.7272727272727272 0.502 :  0.7272727272727272 0.504 :  0.7272727272727272 0.506 :  0.7272727272727272 0.508 :  0.7272727272727272 0.51 :  0.72 0.512 :  0.72 0.514 :  0.72 0.516 :  0.7241379310344828 0.518 :  0.7241379310344828 0.52 :  0.7241379310344828 0.522 :  0.7283236994219653 0.524 :  0.7283236994219653 0.526 :  0.7283236994219653 0.528 :  0.7283236994219653 0.53 :  0.7325581395348837 0.532 :  0.7325581395348837 0.534 :  0.7251461988304093 0.536 :  0.7251461988304093 0.538 :  0.7251461988304093 0.54 :  0.7251461988304093 0.542 :  0.7251461988304093 0.544 :  0.7251461988304093 0.546 :  0.7251461988304093 0.548 :  0.7251461988304093 0.55 :  0.7251461988304093 0.552 :  0.7251461988304093 0.554 :  0.7294117647058822 0.556 :  0.7294117647058822 0.558 :  0.7294117647058822 0.56 :  0.7294117647058822 0.562 :  0.7294117647058822 0.564 :  0.7218934911242604 0.566 :  0.7218934911242604 0.568 :  0.7218934911242604 0.57 :  0.7261904761904763 0.572 :  0.7261904761904763 0.574 :  0.7261904761904763 0.576 :  0.7261904761904763 0.578 :  0.7261904761904763 0.58 :  0.7305389221556886 0.582 :  0.7305389221556886 0.584 :  0.7305389221556886 0.586 :  0.7228915662650603 0.588 :  0.7228915662650603 0.59 :  0.7228915662650603 0.592 :  0.7151515151515152 0.594 :  0.7151515151515152 0.596 :  0.7151515151515152 0.598 :  0.7151515151515152 0.6 :  0.7151515151515152 0.602 :  0.7151515151515152 0.604 :  0.7151515151515152 0.606 :  0.7151515151515152 0.608 :  0.7151515151515152 0.61 :  0.7151515151515152 0.612 :  0.7151515151515152 0.614 :  0.7151515151515152 0.616 :  0.7151515151515152 0.618 :  0.7195121951219512 0.62 :  0.7195121951219512 0.622 :  0.7195121951219512 0.624 :  0.7195121951219512 0.626 :  0.7195121951219512 0.628 :  0.7195121951219512 0.63 :  0.7195121951219512 0.632 :  0.7239263803680982 0.634 :  0.7239263803680982 0.636 :  0.7239263803680982 0.638 :  0.7239263803680982 0.64 :  0.7239263803680982 0.642 :  0.7160493827160495 0.644 :  0.7080745341614907 0.646 :  0.7125 0.648 :  0.7125 0.65 :  0.7125 0.652 :  0.7125 0.654 :  0.7125 0.656 :  0.7125 0.658 :  0.7125 0.66 :  0.7125 0.662 :  0.7125 0.664 :  0.7125 0.666 :  0.7125 0.668 :  0.7125 0.67 :  0.7169811320754716 0.672 :  0.7169811320754716 0.674 :  0.7215189873417722 0.676 :  0.7215189873417722 0.678 :  0.7215189873417722 0.68 :  0.7133757961783439 0.682 :  0.7133757961783439 0.684 :  0.7133757961783439 0.686 :  0.7051282051282051 0.688 :  0.7051282051282051 0.69 :  0.6967741935483871 0.692 :  0.7012987012987014 0.694 :  0.7012987012987014 0.696 :  0.6928104575163399 0.698 :  0.6928104575163399 0.7 :  0.6842105263157895 0.702 :  0.6842105263157895 0.704 :  0.68 0.706 :  0.68 0.708 :  0.68 0.71 :  0.68 0.712 :  0.6845637583892616 0.714 :  0.6845637583892616 0.716 :  0.6845637583892616 0.718 :  0.6845637583892616 0.72 :  0.6756756756756757 0.722 :  0.6666666666666667 0.724 :  0.6666666666666667 0.726 :  0.6666666666666667 0.728 :  0.6575342465753425 0.73 :  0.6575342465753425 0.732 :  0.6620689655172414 0.734 :  0.6620689655172414 0.736 :  0.6620689655172414 0.738 :  0.6620689655172414 0.74 :  0.6527777777777778 0.742 :  0.6338028169014084 0.744 :  0.6338028169014084 0.746 :  0.624113475177305 0.748 :  0.624113475177305 0.75 :  0.6142857142857143 0.752 :  0.6142857142857143 0.754 :  0.60431654676259 0.756 :  0.60431654676259 0.758 :  0.60431654676259 0.76 :  0.60431654676259 0.762 :  0.60431654676259 0.764 :  0.60431654676259 0.766 :  0.60431654676259 0.768 :  0.60431654676259 0.77 :  0.60431654676259 0.772 :  0.60431654676259 0.774 :  0.60431654676259 0.776 :  0.60431654676259 0.778 :  0.60431654676259 0.78 :  0.6086956521739131 0.782 :  0.6086956521739131 0.784 :  0.6086956521739131 0.786 :  0.6086956521739131 0.788 :  0.6086956521739131 0.79 :  0.6131386861313869 0.792 :  0.6131386861313869 0.794 :  0.6029411764705882 0.796 :  0.6074074074074074 0.798 :  0.6074074074074074 0.8 :  0.6074074074074074 0.802 :  0.6074074074074074 0.804 :  0.6074074074074074 0.806 :  0.6074074074074074 0.808 :  0.6165413533834586 0.81 :  0.6106870229007634 0.812 :  0.6106870229007634 0.814 :  0.6106870229007634 0.816 :  0.6106870229007634 0.818 :  0.6046511627906976 0.82 :  0.6046511627906976 0.822 :  0.6046511627906976 0.824 :  0.5937499999999999 0.826 :  0.5826771653543308 0.828 :  0.5826771653543308 0.83 :  0.5760000000000001 0.832 :  0.5760000000000001 0.834 :  0.5645161290322581 0.836 :  0.5528455284552846 0.838 :  0.5528455284552846 0.84 :  0.5528455284552846 0.842 :  0.5573770491803278 max_f1 =  0.7344632768361582\n715\n238\nEpoch 1/50\n23/23 [==============================] - 21s 219ms/step - loss: 0.6382 - accuracy: 0.6011 - recall_at_precision_3: 0.0335 - val_loss: 0.5551 - val_accuracy: 0.7521 - val_recall_at_precision_3: 0.3553\nEpoch 2/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.5436 - accuracy: 0.7495 - recall_at_precision_3: 0.1394 - val_loss: 0.5427 - val_accuracy: 0.7395 - val_recall_at_precision_3: 0.4868\nEpoch 3/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.5202 - accuracy: 0.7544 - recall_at_precision_3: 0.4579 - val_loss: 0.5583 - val_accuracy: 0.7479 - val_recall_at_precision_3: 0.4211\nEpoch 4/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.4651 - accuracy: 0.7797 - recall_at_precision_3: 0.5596 - val_loss: 0.5122 - val_accuracy: 0.7899 - val_recall_at_precision_3: 0.3553\nEpoch 5/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4659 - accuracy: 0.7971 - recall_at_precision_3: 0.6138 - val_loss: 0.4999 - val_accuracy: 0.7605 - val_recall_at_precision_3: 0.4079\nEpoch 6/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.4558 - accuracy: 0.7863 - recall_at_precision_3: 0.6639 - val_loss: 0.5008 - val_accuracy: 0.7731 - val_recall_at_precision_3: 0.6447\nEpoch 7/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.4024 - accuracy: 0.8291 - recall_at_precision_3: 0.8015 - val_loss: 0.5245 - val_accuracy: 0.7605 - val_recall_at_precision_3: 0.4605\nEpoch 8/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.4274 - accuracy: 0.8090 - recall_at_precision_3: 0.7079 - val_loss: 0.5686 - val_accuracy: 0.7521 - val_recall_at_precision_3: 0.2763\nEpoch 9/50\n23/23 [==============================] - 1s 43ms/step - loss: 0.3498 - accuracy: 0.8631 - recall_at_precision_3: 0.8500 - val_loss: 0.5719 - val_accuracy: 0.7815 - val_recall_at_precision_3: 0.4079\nEpoch 10/50\n23/23 [==============================] - 1s 37ms/step - loss: 0.3362 - accuracy: 0.8567 - recall_at_precision_3: 0.8465 - val_loss: 0.6332 - val_accuracy: 0.7311 - val_recall_at_precision_3: 0.2368\nEpoch 11/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.3499 - accuracy: 0.8551 - recall_at_precision_3: 0.7720 - val_loss: 0.6107 - val_accuracy: 0.7605 - val_recall_at_precision_3: 0.4474\nEpoch 12/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2647 - accuracy: 0.9127 - recall_at_precision_3: 0.9286 - val_loss: 0.6139 - val_accuracy: 0.7563 - val_recall_at_precision_3: 0.4605\nEpoch 13/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2016 - accuracy: 0.9382 - recall_at_precision_3: 0.9274 - val_loss: 0.6180 - val_accuracy: 0.7983 - val_recall_at_precision_3: 0.6184\nEpoch 14/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2386 - accuracy: 0.9119 - recall_at_precision_3: 0.9328 - val_loss: 0.8305 - val_accuracy: 0.7689 - val_recall_at_precision_3: 0.4605\nEpoch 15/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2512 - accuracy: 0.9077 - recall_at_precision_3: 0.9170 - val_loss: 0.6965 - val_accuracy: 0.7269 - val_recall_at_precision_3: 0.6842\nEpoch 16/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.2791 - accuracy: 0.8874 - recall_at_precision_3: 0.9471 - val_loss: 0.8271 - val_accuracy: 0.7647 - val_recall_at_precision_3: 0.0000e+00\nEpoch 17/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1172 - accuracy: 0.9567 - recall_at_precision_3: 0.9952 - val_loss: 1.0618 - val_accuracy: 0.7563 - val_recall_at_precision_3: 0.4079\nEpoch 18/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.1128 - accuracy: 0.9600 - recall_at_precision_3: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.7773 - val_recall_at_precision_3: 0.6053\nEpoch 19/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0526 - accuracy: 0.9805 - recall_at_precision_3: 1.0000 - val_loss: 1.1801 - val_accuracy: 0.7101 - val_recall_at_precision_3: 0.0000e+00\nEpoch 20/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0412 - accuracy: 0.9879 - recall_at_precision_3: 1.0000 - val_loss: 1.4453 - val_accuracy: 0.7479 - val_recall_at_precision_3: 0.0000e+00\nEpoch 21/50\n23/23 [==============================] - 1s 36ms/step - loss: 0.0348 - accuracy: 0.9865 - recall_at_precision_3: 1.0000 - val_loss: 1.6552 - val_accuracy: 0.7353 - val_recall_at_precision_3: 0.0000e+00\nEpoch 22/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0075 - accuracy: 0.9978 - recall_at_precision_3: 1.0000 - val_loss: 1.9936 - val_accuracy: 0.7269 - val_recall_at_precision_3: 0.0000e+00\nEpoch 23/50\n23/23 [==============================] - 1s 42ms/step - loss: 0.0258 - accuracy: 0.9909 - recall_at_precision_3: 0.9972 - val_loss: 1.5282 - val_accuracy: 0.7773 - val_recall_at_precision_3: 0.0000e+00\nEpoch 24/50\n23/23 [==============================] - 1s 34ms/step - loss: 0.0909 - accuracy: 0.9659 - recall_at_precision_3: 1.0000 - val_loss: 1.3225 - val_accuracy: 0.7563 - val_recall_at_precision_3: 0.1974\nEpoch 25/50\n23/23 [==============================] - 1s 35ms/step - loss: 0.0293 - accuracy: 0.9938 - recall_at_precision_3: 0.9978 - val_loss: 1.6009 - val_accuracy: 0.7185 - val_recall_at_precision_3: 0.0000e+00\n238\n238\n[[0.7942105 ]\n [0.9747702 ]\n [0.09946949]\n [0.9750851 ]\n [0.9714731 ]\n [0.9681028 ]\n [0.9599505 ]\n [0.9758211 ]\n [0.04325675]\n [0.13295572]]\n0.122 :  0.5523012552301255 0.124 :  0.5523012552301255 0.126 :  0.5523012552301255 0.128 :  0.5593220338983051 0.13 :  0.5593220338983051 0.132 :  0.5531914893617021 0.134 :  0.5579399141630902 0.136 :  0.560344827586207 0.138 :  0.5652173913043478 0.14 :  0.5701754385964911 0.142 :  0.5701754385964911 0.144 :  0.5701754385964911 0.146 :  0.5701754385964911 0.148 :  0.5638766519823789 0.15 :  0.5663716814159292 0.152 :  0.5688888888888889 0.154 :  0.5688888888888889 0.156 :  0.5688888888888889 0.158 :  0.5739910313901345 0.16 :  0.5727272727272726 0.162 :  0.5727272727272726 0.164 :  0.5753424657534246 0.166 :  0.5779816513761468 0.168 :  0.5779816513761468 0.17 :  0.5779816513761468 0.172 :  0.5714285714285714 0.174 :  0.5740740740740741 0.176 :  0.5794392523364487 0.178 :  0.5821596244131455 0.18 :  0.5849056603773585 0.182 :  0.5876777251184834 0.184 :  0.5876777251184834 0.186 :  0.5876777251184834 0.188 :  0.5876777251184834 0.19 :  0.5876777251184834 0.192 :  0.5876777251184834 0.194 :  0.5876777251184834 0.196 :  0.5904761904761905 0.198 :  0.5933014354066986 0.2 :  0.5933014354066986 0.202 :  0.5865384615384616 0.204 :  0.5865384615384616 0.206 :  0.5893719806763286 0.208 :  0.5893719806763286 0.21 :  0.5922330097087379 0.212 :  0.5922330097087379 0.214 :  0.5922330097087379 0.216 :  0.5922330097087379 0.218 :  0.5951219512195122 0.22 :  0.5951219512195122 0.222 :  0.5951219512195122 0.224 :  0.5980392156862745 0.226 :  0.5980392156862745 0.228 :  0.6009852216748769 0.23 :  0.6009852216748769 0.232 :  0.6069651741293531 0.234 :  0.6069651741293531 0.236 :  0.6100000000000001 0.238 :  0.6100000000000001 0.24 :  0.6100000000000001 0.242 :  0.6030150753768844 0.244 :  0.6030150753768844 0.246 :  0.6060606060606061 0.248 :  0.598984771573604 0.25 :  0.598984771573604 0.252 :  0.598984771573604 0.254 :  0.598984771573604 0.256 :  0.598984771573604 0.258 :  0.6020408163265306 0.26 :  0.6020408163265306 0.262 :  0.6020408163265306 0.264 :  0.6051282051282052 0.266 :  0.6051282051282052 0.268 :  0.6051282051282052 0.27 :  0.6051282051282052 0.272 :  0.6082474226804124 0.274 :  0.6082474226804124 0.276 :  0.6082474226804124 0.278 :  0.6113989637305699 0.28 :  0.6113989637305699 0.282 :  0.6041666666666667 0.284 :  0.6041666666666667 0.286 :  0.5968586387434555 0.288 :  0.5968586387434555 0.29 :  0.5968586387434555 0.292 :  0.603174603174603 0.294 :  0.603174603174603 0.296 :  0.603174603174603 0.298 :  0.603174603174603 0.3 :  0.603174603174603 0.302 :  0.603174603174603 0.304 :  0.603174603174603 0.306 :  0.603174603174603 0.308 :  0.603174603174603 0.31 :  0.6063829787234042 0.312 :  0.6063829787234042 0.314 :  0.6063829787234042 0.316 :  0.6063829787234042 0.318 :  0.6063829787234042 0.32 :  0.6063829787234042 0.322 :  0.6063829787234042 0.324 :  0.6063829787234042 0.326 :  0.6063829787234042 0.328 :  0.6063829787234042 0.33 :  0.6063829787234042 0.332 :  0.6063829787234042 0.334 :  0.6063829787234042 0.336 :  0.6063829787234042 0.338 :  0.6063829787234042 0.34 :  0.6063829787234042 0.342 :  0.6063829787234042 0.344 :  0.6063829787234042 0.346 :  0.6096256684491977 0.348 :  0.6054054054054054 0.35 :  0.6054054054054054 0.352 :  0.6086956521739131 0.354 :  0.6086956521739131 0.356 :  0.6086956521739131 0.358 :  0.6086956521739131 0.36 :  0.6086956521739131 0.362 :  0.6086956521739131 0.364 :  0.6086956521739131 0.366 :  0.6086956521739131 0.368 :  0.6120218579234972 0.37 :  0.6120218579234972 0.372 :  0.6120218579234972 0.374 :  0.6120218579234972 0.376 :  0.6120218579234972 0.378 :  0.6120218579234972 0.38 :  0.6153846153846154 0.382 :  0.6153846153846154 0.384 :  0.6153846153846154 0.386 :  0.6153846153846154 0.388 :  0.6153846153846154 0.39 :  0.6153846153846154 0.392 :  0.6077348066298343 0.394 :  0.6077348066298343 0.396 :  0.6077348066298343 0.398 :  0.6077348066298343 0.4 :  0.6077348066298343 0.402 :  0.6111111111111112 0.404 :  0.6111111111111112 0.406 :  0.6111111111111112 0.408 :  0.6111111111111112 0.41 :  0.6111111111111112 0.412 :  0.6111111111111112 0.414 :  0.6111111111111112 0.416 :  0.6111111111111112 0.418 :  0.6111111111111112 0.42 :  0.6111111111111112 0.422 :  0.6111111111111112 0.424 :  0.6111111111111112 0.426 :  0.6179775280898876 0.428 :  0.6179775280898876 0.43 :  0.6179775280898876 0.432 :  0.6179775280898876 0.434 :  0.6179775280898876 0.436 :  0.6179775280898876 0.438 :  0.6179775280898876 0.44 :  0.6179775280898876 0.442 :  0.6179775280898876 0.444 :  0.6179775280898876 0.446 :  0.6179775280898876 0.448 :  0.6214689265536724 0.45 :  0.6214689265536724 0.452 :  0.6250000000000001 0.454 :  0.6250000000000001 0.456 :  0.6250000000000001 0.458 :  0.6250000000000001 0.46 :  0.6250000000000001 0.462 :  0.6250000000000001 0.464 :  0.6250000000000001 0.466 :  0.6250000000000001 0.468 :  0.6250000000000001 0.47 :  0.6250000000000001 0.472 :  0.6250000000000001 0.474 :  0.6250000000000001 0.476 :  0.6250000000000001 0.478 :  0.6250000000000001 0.48 :  0.6250000000000001 0.482 :  0.6250000000000001 0.484 :  0.6250000000000001 0.486 :  0.6250000000000001 0.488 :  0.6250000000000001 0.49 :  0.6250000000000001 0.492 :  0.6250000000000001 0.494 :  0.6250000000000001 0.496 :  0.6250000000000001 0.498 :  0.6250000000000001 0.5 :  0.6285714285714287 0.502 :  0.6321839080459769 0.504 :  0.6321839080459769 0.506 :  0.6321839080459769 0.508 :  0.6321839080459769 0.51 :  0.6321839080459769 0.512 :  0.6321839080459769 0.514 :  0.6358381502890174 0.516 :  0.6358381502890174 0.518 :  0.6358381502890174 0.52 :  0.6358381502890174 0.522 :  0.6358381502890174 0.524 :  0.6358381502890174 0.526 :  0.6358381502890174 0.528 :  0.6395348837209303 0.53 :  0.6395348837209303 0.532 :  0.6395348837209303 0.534 :  0.6395348837209303 0.536 :  0.6395348837209303 0.538 :  0.6395348837209303 0.54 :  0.6395348837209303 0.542 :  0.6395348837209303 0.544 :  0.6395348837209303 0.546 :  0.6395348837209303 0.548 :  0.6395348837209303 0.55 :  0.6432748538011696 0.552 :  0.6432748538011696 0.554 :  0.6432748538011696 0.556 :  0.6432748538011696 0.558 :  0.6432748538011696 0.56 :  0.6432748538011696 0.562 :  0.6432748538011696 0.564 :  0.6432748538011696 0.566 :  0.6432748538011696 0.568 :  0.6432748538011696 0.57 :  0.6432748538011696 0.572 :  0.6432748538011696 0.574 :  0.6432748538011696 0.576 :  0.6432748538011696 0.578 :  0.6432748538011696 0.58 :  0.6432748538011696 0.582 :  0.6432748538011696 0.584 :  0.6432748538011696 0.586 :  0.6432748538011696 0.588 :  0.6432748538011696 0.59 :  0.6470588235294118 0.592 :  0.6470588235294118 0.594 :  0.6470588235294118 0.596 :  0.6470588235294118 0.598 :  0.6470588235294118 0.6 :  0.650887573964497 0.602 :  0.6586826347305389 0.604 :  0.6586826347305389 0.606 :  0.6586826347305389 0.608 :  0.6586826347305389 0.61 :  0.6586826347305389 0.612 :  0.6586826347305389 0.614 :  0.6586826347305389 0.616 :  0.6586826347305389 0.618 :  0.6586826347305389 0.62 :  0.6586826347305389 0.622 :  0.6586826347305389 0.624 :  0.6586826347305389 0.626 :  0.6626506024096386 0.628 :  0.6626506024096386 0.63 :  0.6626506024096386 0.632 :  0.6626506024096386 0.634 :  0.6626506024096386 0.636 :  0.6626506024096386 0.638 :  0.6626506024096386 0.64 :  0.6626506024096386 0.642 :  0.6626506024096386 0.644 :  0.6626506024096386 0.646 :  0.6626506024096386 0.648 :  0.6626506024096386 0.65 :  0.6626506024096386 0.652 :  0.6626506024096386 0.654 :  0.6626506024096386 0.656 :  0.6626506024096386 0.658 :  0.6626506024096386 0.66 :  0.6626506024096386 0.662 :  0.6626506024096386 0.664 :  0.6626506024096386 0.666 :  0.6626506024096386 0.668 :  0.6666666666666666 0.67 :  0.6666666666666666 0.672 :  0.6707317073170731 0.674 :  0.6707317073170731 0.676 :  0.6707317073170731 0.678 :  0.6707317073170731 0.68 :  0.6707317073170731 0.682 :  0.6707317073170731 0.684 :  0.6707317073170731 0.686 :  0.6707317073170731 0.688 :  0.6707317073170731 0.69 :  0.6707317073170731 0.692 :  0.6707317073170731 0.694 :  0.6707317073170731 0.696 :  0.6707317073170731 0.698 :  0.6707317073170731 0.7 :  0.6707317073170731 0.702 :  0.6707317073170731 0.704 :  0.6707317073170731 0.706 :  0.6707317073170731 0.708 :  0.6707317073170731 0.71 :  0.6707317073170731 0.712 :  0.6707317073170731 0.714 :  0.6707317073170731 0.716 :  0.6707317073170731 0.718 :  0.6707317073170731 0.72 :  0.6707317073170731 0.722 :  0.6707317073170731 0.724 :  0.6748466257668712 0.726 :  0.6748466257668712 0.728 :  0.6748466257668712 0.73 :  0.6748466257668712 0.732 :  0.6748466257668712 0.734 :  0.6748466257668712 0.736 :  0.6748466257668712 0.738 :  0.6748466257668712 0.74 :  0.6748466257668712 0.742 :  0.6748466257668712 0.744 :  0.6748466257668712 0.746 :  0.6748466257668712 0.748 :  0.6748466257668712 0.75 :  0.6748466257668712 0.752 :  0.6748466257668712 0.754 :  0.6790123456790124 0.756 :  0.6790123456790124 0.758 :  0.6790123456790124 0.76 :  0.6790123456790124 0.762 :  0.6832298136645962 0.764 :  0.6832298136645962 0.766 :  0.6832298136645962 0.768 :  0.6832298136645962 0.77 :  0.6832298136645962 0.772 :  0.6832298136645962 0.774 :  0.6832298136645962 0.776 :  0.6832298136645962 0.778 :  0.6832298136645962 0.78 :  0.6832298136645962 0.782 :  0.6832298136645962 0.784 :  0.6832298136645962 0.786 :  0.6832298136645962 0.788 :  0.6832298136645962 0.79 :  0.6832298136645962 0.792 :  0.6875 0.794 :  0.6875 0.796 :  0.6918238993710693 0.798 :  0.6918238993710693 0.8 :  0.6918238993710693 0.802 :  0.6918238993710693 0.804 :  0.6918238993710693 0.806 :  0.6918238993710693 0.808 :  0.6918238993710693 0.81 :  0.6918238993710693 0.812 :  0.6918238993710693 0.814 :  0.6918238993710693 0.816 :  0.6918238993710693 0.818 :  0.6918238993710693 0.82 :  0.6918238993710693 0.822 :  0.6918238993710693 0.824 :  0.6835443037974684 0.826 :  0.6835443037974684 0.828 :  0.6835443037974684 0.83 :  0.6835443037974684 0.832 :  0.6835443037974684 0.834 :  0.6835443037974684 0.836 :  0.6835443037974684 0.838 :  0.6878980891719745 0.84 :  0.6878980891719745 0.842 :  0.6794871794871794 0.844 :  0.6794871794871794 0.846 :  0.6794871794871794 0.848 :  0.6794871794871794 0.85 :  0.6794871794871794 0.852 :  0.6794871794871794 0.854 :  0.6794871794871794 0.856 :  0.6838709677419355 0.858 :  0.6838709677419355 0.86 :  0.6838709677419355 0.862 :  0.6883116883116882 0.864 :  0.6928104575163399 0.866 :  0.6842105263157895 0.868 :  0.6887417218543047 0.87 :  0.6887417218543047 0.872 :  0.6887417218543047 0.874 :  0.6887417218543047 0.876 :  0.6887417218543047 0.878 :  0.6933333333333334 0.88 :  0.6933333333333334 0.882 :  0.6933333333333334 0.884 :  0.6933333333333334 0.886 :  0.6933333333333334 0.888 :  0.6933333333333334 0.89 :  0.6933333333333334 0.892 :  0.6933333333333334 0.894 :  0.6845637583892618 0.896 :  0.6845637583892618 0.898 :  0.6756756756756757 max_f1 =  0.6933333333333334\n","output_type":"stream"}]},{"cell_type":"code","source":"f1 = []\nacc = {}\nrec = {}\nprec = {}\n\nfor i in range(len(f1_vals[0])):\n    f1_avg = (f1_vals[0][i]+f1_vals[1][i]+f1_vals[2][i]+f1_vals[3][i])/4\n    if(f1_avg>.55):\n        f1.append((f1_avg, thresholds[i]))\n#   acc[thresholds[i]]=(acc_vals[0][i]+acc_vals[1][i]+acc_vals[2][i]+acc_vals[3][i])/4\n#   rec[thresholds[i]]=(recall_vals[0][i]+recall_vals[1][i]+recall_vals[2][i]+recall_vals[3][i])/4\n#   prec[thresholds[i]]=(precision_vals[0][i]+precision_vals[1][i]+precision_vals[2][i]+precision_vals[3][i])/4\nf1.sort(reverse=True)\nfor score, threshold in f1:\n    print(round(threshold, 3), \": \", score)\n# print(acc)\n# print(rec)\n# print(prec)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T13:50:28.063976Z","iopub.execute_input":"2021-05-29T13:50:28.064279Z","iopub.status.idle":"2021-05-29T13:50:28.180472Z","shell.execute_reply.started":"2021-05-29T13:50:28.064251Z","shell.execute_reply":"2021-05-29T13:50:28.178949Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0.106 :  0.6778910059645413\n0.104 :  0.6778910059645413\n0.102 :  0.6778910059645413\n0.108 :  0.6761844066785927\n0.11 :  0.675237436981623\n0.112 :  0.6744683620237164\n0.116 :  0.6741915070956987\n0.114 :  0.6741915070956987\n0.118 :  0.6730945763496985\n0.122 :  0.6719962609323892\n0.12 :  0.6719962609323892\n0.126 :  0.6709443675242546\n0.124 :  0.6709443675242546\n0.128 :  0.6699828290627161\n0.132 :  0.6693091337157155\n0.13 :  0.6693091337157155\n0.134 :  0.6685364185545113\n0.136 :  0.6681925759815108\n0.144 :  0.6679298329442012\n0.142 :  0.6679298329442012\n0.14 :  0.6679298329442012\n0.138 :  0.6679298329442012\n0.146 :  0.6676010428915948\n0.148 :  0.6668304985124037\n0.15 :  0.6661700584392372\n0.154 :  0.6661534316768003\n0.152 :  0.6661534316768003\n0.156 :  0.6652237327410089\n0.158 :  0.6651162243553548\n0.16 :  0.6646478610667768\n0.162 :  0.6636879254431756\n0.164 :  0.6624412787145555\n0.166 :  0.6620949383774508\n0.17 :  0.6609780858581656\n0.168 :  0.6609780858581656\n0.172 :  0.6608897778397976\n0.178 :  0.6600083927761827\n0.176 :  0.6600083927761827\n0.174 :  0.6600083927761827\n0.18 :  0.6599267267900659\n0.182 :  0.6592675327216285\n0.184 :  0.6589661100064126\n0.186 :  0.6586728892698421\n0.188 :  0.6581948315352188\n0.19 :  0.6576317684721558\n0.192 :  0.657515299802228\n0.196 :  0.6574366772690099\n0.194 :  0.6574366772690099\n0.198 :  0.6571779737432504\n0.202 :  0.6567352896798767\n0.2 :  0.6567352896798767\n0.204 :  0.6565679536691672\n0.206 :  0.6563684666358244\n0.21 :  0.6550052736669805\n0.208 :  0.6550052736669805\n0.214 :  0.6549400080358618\n0.212 :  0.6549400080358618\n0.216 :  0.6542055447590258\n0.218 :  0.6541233619845352\n0.22 :  0.6527558412483883\n0.222 :  0.6521627800642427\n0.228 :  0.6511334955742664\n0.226 :  0.6511334955742664\n0.224 :  0.6511334955742664\n0.232 :  0.6507537487388233\n0.23 :  0.6507537487388233\n0.234 :  0.6501196101841725\n0.236 :  0.6499117778241588\n0.24 :  0.649793069372117\n0.238 :  0.649793069372117\n0.244 :  0.6494959808397344\n0.242 :  0.6494959808397344\n0.246 :  0.648033528352376\n0.248 :  0.6477670494272143\n0.25 :  0.6469452351053213\n0.258 :  0.6465283288373512\n0.256 :  0.6465283288373512\n0.254 :  0.6465283288373512\n0.252 :  0.6465283288373512\n0.264 :  0.6461528653746395\n0.262 :  0.6461528653746395\n0.26 :  0.6461528653746395\n0.268 :  0.6461352006510923\n0.266 :  0.6461352006510923\n0.27 :  0.6458224726202365\n0.272 :  0.6457757872794335\n0.274 :  0.6453430497397777\n0.276 :  0.6451185828486442\n0.28 :  0.6440898532361947\n0.278 :  0.6440898532361947\n0.282 :  0.6440077514299549\n0.286 :  0.6438253029716445\n0.284 :  0.6438253029716445\n0.288 :  0.6437235528698944\n0.29 :  0.6434434408250764\n0.292 :  0.6433335507151863\n0.294 :  0.6431184969517454\n0.296 :  0.643060485581517\n0.3 :  0.642922418520373\n0.298 :  0.642922418520373\n0.302 :  0.6427059683039227\n0.306 :  0.6426658076613524\n0.304 :  0.6426658076613524\n0.31 :  0.6419216545784324\n0.308 :  0.6419216545784324\n0.312 :  0.6418364030865313\n0.316 :  0.6416684407101036\n0.314 :  0.6416684407101036\n0.318 :  0.6414938840839342\n0.32 :  0.6414774031029862\n0.324 :  0.6412843529099359\n0.322 :  0.6412843529099359\n0.328 :  0.6411641284910207\n0.326 :  0.6411641284910207\n0.332 :  0.6401478683284191\n0.33 :  0.6401478683284191\n0.334 :  0.6390932449767126\n0.338 :  0.6390144303487177\n0.336 :  0.6390144303487177\n0.344 :  0.6387866821160697\n0.342 :  0.6387866821160697\n0.34 :  0.6387866821160697\n0.354 :  0.6385677330714836\n0.352 :  0.6385677330714836\n0.35 :  0.6385677330714836\n0.348 :  0.6385677330714836\n0.346 :  0.6385677330714836\n0.358 :  0.6383675964018676\n0.356 :  0.6383675964018676\n0.368 :  0.6380866616553473\n0.366 :  0.6380866616553473\n0.364 :  0.6380866616553473\n0.362 :  0.6380866616553473\n0.36 :  0.6380866616553473\n0.37 :  0.6373472384160297\n0.372 :  0.6366736460699268\n0.374 :  0.6362854473121629\n0.382 :  0.6352041514528085\n0.38 :  0.6352041514528085\n0.378 :  0.6352041514528085\n0.376 :  0.6352041514528085\n0.384 :  0.635099356013506\n0.386 :  0.6348280458319747\n0.394 :  0.6345569661603684\n0.392 :  0.6345569661603684\n0.39 :  0.6345569661603684\n0.388 :  0.6345569661603684\n0.396 :  0.6345270116911617\n0.398 :  0.6342940200788597\n0.4 :  0.6320781905484674\n0.404 :  0.630615082721939\n0.402 :  0.630615082721939\n0.41 :  0.6302955338497586\n0.408 :  0.6302955338497586\n0.406 :  0.6302955338497586\n0.414 :  0.6291896948703833\n0.412 :  0.6291896948703833\n0.418 :  0.6290135696257649\n0.416 :  0.6290135696257649\n0.422 :  0.6288250326725221\n0.42 :  0.6288250326725221\n0.426 :  0.6280350746848524\n0.424 :  0.6280350746848524\n0.432 :  0.6277260952676351\n0.43 :  0.6277260952676351\n0.428 :  0.6277260952676351\n0.434 :  0.6276296981113513\n0.436 :  0.6274066971556329\n0.438 :  0.6267460206228633\n0.44 :  0.6263576573973302\n0.444 :  0.6258934554589909\n0.442 :  0.6258934554589909\n0.448 :  0.6256409302064656\n0.446 :  0.6256409302064656\n0.45 :  0.6256057684624431\n0.458 :  0.6242388607397326\n0.456 :  0.6242388607397326\n0.454 :  0.6242388607397326\n0.452 :  0.6242388607397326\n0.46 :  0.6240316211707908\n0.462 :  0.623824324321703\n0.466 :  0.6230112796460265\n0.464 :  0.6230112796460265\n0.468 :  0.6224847990973484\n0.472 :  0.62239980844632\n0.47 :  0.62239980844632\n0.474 :  0.6213979612175745\n0.476 :  0.6205538850972554\n0.478 :  0.6200255618149504\n0.48 :  0.61983315368983\n0.482 :  0.6196652391879435\n0.486 :  0.6190895968104315\n0.484 :  0.6190895968104315\n0.488 :  0.6180031309475208\n0.49 :  0.6174937930867398\n0.492 :  0.617278970315526\n0.496 :  0.6171293294535947\n0.494 :  0.6171293294535947\n0.498 :  0.6169590877103193\n0.5 :  0.6153534483796907\n0.504 :  0.6139512391903843\n0.502 :  0.6139512391903843\n0.506 :  0.6134290371824334\n0.508 :  0.6134051114479133\n0.51 :  0.613313531564089\n0.514 :  0.6131958401979276\n0.512 :  0.6131958401979276\n0.516 :  0.6128646420513211\n0.518 :  0.6127581641659312\n0.52 :  0.6118728003149023\n0.522 :  0.6118458816899056\n0.528 :  0.6117046387520525\n0.526 :  0.6117046387520525\n0.524 :  0.6117046387520525\n0.53 :  0.6110712270320444\n0.532 :  0.6110589658191652\n0.534 :  0.6109357090224333\n0.536 :  0.6105939240063784\n0.538 :  0.6104127859390656\n0.54 :  0.6094523618372061\n0.542 :  0.6094406404315751\n0.544 :  0.6093781013446458\n0.548 :  0.6088772498971851\n0.546 :  0.6088772498971851\n0.552 :  0.6079944815356031\n0.55 :  0.6079944815356031\n0.556 :  0.6070333239012888\n0.554 :  0.6070333239012888\n0.558 :  0.6069651482808907\n0.56 :  0.6069128360816859\n0.562 :  0.6067805478541595\n0.564 :  0.6064652303609691\n0.568 :  0.6064137423576739\n0.566 :  0.6064137423576739\n0.57 :  0.6060904525606998\n0.572 :  0.6060300806698522\n0.574 :  0.6055541841837689\n0.578 :  0.6049059560074509\n0.576 :  0.6049059560074509\n0.586 :  0.604052059052059\n0.584 :  0.604052059052059\n0.582 :  0.604052059052059\n0.58 :  0.604052059052059\n0.59 :  0.6026470690896263\n0.588 :  0.6026470690896263\n0.592 :  0.601615249092289\n0.594 :  0.6008579826495097\n0.598 :  0.600808636486174\n0.596 :  0.600808636486174\n0.6 :  0.6006529129379421\n0.602 :  0.6004022863715262\n0.608 :  0.6003303232166615\n0.606 :  0.6003303232166615\n0.604 :  0.6003303232166615\n0.61 :  0.6000207256934416\n0.612 :  0.6000076725672179\n0.614 :  0.5998423590225281\n0.618 :  0.5997974273187179\n0.616 :  0.5997974273187179\n0.62 :  0.5990372430858332\n0.626 :  0.5990259584419766\n0.624 :  0.5990259584419766\n0.622 :  0.5990259584419766\n0.63 :  0.5983377787096794\n0.628 :  0.5983377787096794\n0.632 :  0.5982448104879079\n0.634 :  0.5978762395562093\n0.636 :  0.5971759828232819\n0.642 :  0.5968557199292193\n0.64 :  0.5968557199292193\n0.638 :  0.5968557199292193\n0.644 :  0.5967945695553152\n0.65 :  0.5953446204165203\n0.648 :  0.5953446204165203\n0.646 :  0.5953446204165203\n0.652 :  0.5952325630431452\n0.656 :  0.5945714587430864\n0.654 :  0.5945714587430864\n0.658 :  0.5944433047304418\n0.664 :  0.5942805286196702\n0.662 :  0.5942805286196702\n0.66 :  0.5942805286196702\n0.666 :  0.592874848381934\n0.668 :  0.5920676952169921\n0.67 :  0.5917479776253904\n0.672 :  0.590817104860946\n0.674 :  0.59005674138342\n0.676 :  0.5897699501674253\n0.678 :  0.5890158971323618\n0.68 :  0.5887337883814943\n0.682 :  0.5879490032943836\n0.686 :  0.5879389912799663\n0.684 :  0.5879389912799663\n0.688 :  0.5875914794645646\n0.69 :  0.5871889396032854\n0.692 :  0.5859998224038943\n0.694 :  0.5846036532845889\n0.696 :  0.5841113923667773\n0.698 :  0.5833928201732785\n0.7 :  0.5820236095529666\n0.702 :  0.5812437894075198\n0.704 :  0.5811560701092741\n0.706 :  0.580191631587846\n0.708 :  0.5792738569648118\n0.71 :  0.5784170626867151\n0.712 :  0.5781721147381542\n0.714 :  0.5781028146688542\n0.716 :  0.5770114450052999\n0.718 :  0.574948177825144\n0.72 :  0.5731568924445891\n0.722 :  0.5724751027115402\n0.724 :  0.5723677692716145\n0.726 :  0.57023489967632\n0.728 :  0.569221681096681\n0.73 :  0.5687423949366168\n0.732 :  0.5677658800768232\n0.734 :  0.5677337616755487\n0.736 :  0.5674470046369668\n0.738 :  0.5668220046369667\n0.74 :  0.5667631395557331\n0.742 :  0.5657630024919272\n0.744 :  0.5650394365769453\n0.746 :  0.5648186303186651\n0.752 :  0.564005095341342\n0.75 :  0.564005095341342\n0.748 :  0.564005095341342\n0.756 :  0.5626683306520572\n0.754 :  0.5626683306520572\n0.758 :  0.562168218126739\n0.76 :  0.5612706098562521\n0.762 :  0.5591040984149893\n0.764 :  0.5583617258274418\n0.766 :  0.5580156802926225\n0.768 :  0.5565699891067538\n0.77 :  0.5561554850698449\n0.772 :  0.5555307972520014\n0.774 :  0.5549689023695671\n0.776 :  0.5533190388413054\n0.778 :  0.5523322022662408\n0.78 :  0.5504414575721601\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}